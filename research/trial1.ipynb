{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Python Settings (Ok)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\polag\\\\source\\\\repos\\\\automat\\\\sourcecodeanalysis\\\\research'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_folder = f\"test_repo_{uuid.uuid4().hex[:8]}\"  # Creates a name like \"test_repo_a1b2c3d4\"\n",
    "!mkdir repo_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = f\"{repo_folder}/\"\n",
    "\n",
    "repo = Repo.clone_from(\"https://github.com/ram541619/CustomerOnboardFlow\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericLoader.from_filesystem(repo_path,\n",
    "                                        glob = \"**/*\",\n",
    "                                       suffixes=[\".cs\"],\n",
    "                                       parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 17 documents to extracted_texts/\n",
      "Successfully saved 17 documents to extracted_texts/\n",
      "Documents converted to text files in: extracted_texts\n"
     ]
    }
   ],
   "source": [
    "documents = loader.load()\n",
    "# Assuming you've already loaded documents with:\n",
    "# documents = loader.load()\n",
    "\n",
    "# Convert documents to text files\n",
    "def save_documents_as_txt(documents, output_dir=\"extracted_texts\"):\n",
    "    \"\"\"\n",
    "    Save loaded documents as text files in the specified directory\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Saving {len(documents)} documents to {output_dir}/\")\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        # Create a safe filename from the source or use an index\n",
    "        if hasattr(doc, 'metadata') and 'source' in doc.metadata:\n",
    "            # Extract filename from source path\n",
    "            source = doc.metadata['source']\n",
    "            filename = os.path.basename(source).replace('.', '_')\n",
    "        else:\n",
    "            filename = f\"document_{i}\"\n",
    "        \n",
    "        # Ensure filename is unique\n",
    "        filepath = os.path.join(output_dir, f\"{filename}.txt\")\n",
    "        counter = 1\n",
    "        while os.path.exists(filepath):\n",
    "            filepath = os.path.join(output_dir, f\"{filename}_{counter}.txt\")\n",
    "            counter += 1\n",
    "        \n",
    "        # Write document content to file\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            if hasattr(doc, 'page_content'):\n",
    "                f.write(doc.page_content)\n",
    "            else:\n",
    "                f.write(str(doc))\n",
    "    \n",
    "    print(f\"Successfully saved {len(documents)} documents to {output_dir}/\")\n",
    "    return output_dir\n",
    "\n",
    "# Call the function to save documents as text files\n",
    "output_directory = save_documents_as_txt(documents)\n",
    "\n",
    "print(f\"Documents converted to text files in: {output_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 85 text files to consolidate.\n",
      "Successfully consolidated all text files into 'consolidated_repo.txt'.\n",
      "All repository content has been consolidated into: consolidated_repo.txt\n"
     ]
    }
   ],
   "source": [
    "def consolidate_text_files(input_dir=\"extracted_texts\", output_file=\"consolidated_repo.txt\"):\n",
    "    \"\"\"\n",
    "    Consolidate all text files from extracted_texts folder (including subdirectories)\n",
    "    into a single text file.\n",
    "    \"\"\"\n",
    "    # Ensure the input directory exists\n",
    "    if not os.path.exists(input_dir):\n",
    "        print(f\"Error: Directory '{input_dir}' does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    # Find all text files in the directory and its subdirectories\n",
    "    text_files = glob.glob(os.path.join(input_dir, \"**\", \"*.txt\"), recursive=True)\n",
    "    \n",
    "    if not text_files:\n",
    "        print(f\"No text files found in '{input_dir}'.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(text_files)} text files to consolidate.\")\n",
    "    \n",
    "    # Open the output file for writing\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        # Write a header with repository information\n",
    "        outfile.write(\"# Consolidated Repository Content\\n\")\n",
    "        outfile.write(\"Repository: https://uithub.com/ram541619/CustomerOnboardFlow\\n\\n\")\n",
    "        \n",
    "        # Process each text file\n",
    "        for file_path in sorted(text_files):\n",
    "            # Get relative path for file identification\n",
    "            rel_path = os.path.relpath(file_path, input_dir)\n",
    "            \n",
    "            # Write file separator and path information\n",
    "            outfile.write(f\"\\n\\n{'='*80}\\n\")\n",
    "            outfile.write(f\"FILE: {rel_path}\\n\")\n",
    "            outfile.write(f\"{'='*80}\\n\\n\")\n",
    "            \n",
    "            # Read and write the content of the file\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as infile:\n",
    "                    content = infile.read()\n",
    "                    outfile.write(content)\n",
    "            except Exception as e:\n",
    "                outfile.write(f\"[Error reading file: {str(e)}]\")\n",
    "    \n",
    "    print(f\"Successfully consolidated all text files into '{output_file}'.\")\n",
    "    return output_file\n",
    "\n",
    "# Call the function to consolidate all text files\n",
    "consolidated_file = consolidate_text_files()\n",
    "\n",
    "print(f\"All repository content has been consolidated into: {consolidated_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Framework: aspnet (confidence: 15)\n",
      "Found 0 endpoints across 0 files\n",
      "Found 5 potential request/response models\n",
      "Detailed results saved to api_analysis_results.json\n"
     ]
    }
   ],
   "source": [
    "def analyze_consolidated_repo(file_path=\"consolidated_repo.txt\"):\n",
    "    \"\"\"\n",
    "    Analyze the consolidated repository file to extract:\n",
    "    1. API endpoints\n",
    "    2. Request/response body patterns\n",
    "    3. Framework indicators\n",
    "    \"\"\"\n",
    "    # Framework detection patterns\n",
    "    framework_patterns = {\n",
    "        'react': [r'import\\s+React', r'from\\s+[\\'\"]react[\\'\"]', r'ReactDOM', r'<.*Component'],\n",
    "        'angular': [r'@Component', r'@NgModule', r'import.*@angular/core'],\n",
    "        'vue': [r'new\\s+Vue', r'createApp', r'Vue\\.component'],\n",
    "        'express': [r'express\\(\\)', r'app\\.use\\(', r'app\\.(get|post|put|delete)'],\n",
    "        'django': [r'from\\s+django', r'urlpatterns', r'django\\.urls'],\n",
    "        'flask': [r'from\\s+flask\\s+import', r'app\\s*=\\s*Flask', r'@app\\.route'],\n",
    "        'spring': [r'@RestController', r'@RequestMapping', r'@SpringBootApplication'],\n",
    "        'aspnet': [r'using\\s+Microsoft\\.AspNetCore', r'\\[ApiController\\]', r'\\[Route\\('],\n",
    "        'laravel': [r'use\\s+Illuminate', r'extends\\s+Controller', r'Route::'],\n",
    "    }\n",
    "    \n",
    "    # Endpoint detection patterns\n",
    "    endpoint_patterns = {\n",
    "        'rest_api': r'@(GetMapping|PostMapping|PutMapping|DeleteMapping|RequestMapping)\\([\\'\"](.+?)[\\'\"]\\)',\n",
    "        'express': r'(app|router)\\.(get|post|put|delete|patch)\\([\\'\"](.+?)[\\'\"]\\s*,',\n",
    "        'flask': r'@app\\.route\\([\\'\"](.+?)[\\'\"]\\)',\n",
    "        'django': r'path\\([\\'\"](.+?)[\\'\"]\\s*,',\n",
    "        'generic_url': r'(https?://[^\\s\\'\"]+)',\n",
    "        'swagger': r'paths:\\s*\\n(\\s+/[^\\n]+)',\n",
    "        'openapi': r'[\\'\"]/[^\\'\"}]*[\\'\"]:\\s*{',\n",
    "    }\n",
    "    \n",
    "    # Request/response body patterns\n",
    "    body_patterns = {\n",
    "        'json_schema': r'schema\\s*:\\s*{([^}]+)}',\n",
    "        'request_body': r'requestBody\\s*:\\s*{([^}]+)}',\n",
    "        'response_body': r'responses\\s*:\\s*{([^}]+)}',\n",
    "        'typescript_interface': r'interface\\s+(\\w+)\\s*{([^}]+)}',\n",
    "        'class_definition': r'class\\s+(\\w+).*{([^}]+)}',\n",
    "    }\n",
    "    \n",
    "    # Results containers\n",
    "    results = {\n",
    "        'framework_scores': defaultdict(int),\n",
    "        'endpoints': [],\n",
    "        'request_response_models': []\n",
    "    }\n",
    "    \n",
    "    # Current file being processed\n",
    "    current_file = \"\"\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "            \n",
    "            # Split content by file markers\n",
    "            file_sections = re.split(r'={80}\\nFILE:\\s*(.+?)\\n={80}', content)\n",
    "            \n",
    "            # Process each file section\n",
    "            for i in range(1, len(file_sections), 2):\n",
    "                if i < len(file_sections):\n",
    "                    current_file = file_sections[i]\n",
    "                    file_content = file_sections[i+1] if i+1 < len(file_sections) else \"\"\n",
    "                    \n",
    "                    # Detect framework indicators\n",
    "                    for framework, patterns in framework_patterns.items():\n",
    "                        for pattern in patterns:\n",
    "                            matches = re.findall(pattern, file_content)\n",
    "                            results['framework_scores'][framework] += len(matches)\n",
    "                    \n",
    "                    # Detect endpoints\n",
    "                    file_endpoints = []\n",
    "                    for pattern_name, pattern in endpoint_patterns.items():\n",
    "                        matches = re.findall(pattern, file_content)\n",
    "                        if matches:\n",
    "                            if isinstance(matches[0], tuple):\n",
    "                                # Handle tuple results (like from express pattern)\n",
    "                                for match in matches:\n",
    "                                    # Get the last item in the tuple which is typically the endpoint\n",
    "                                    endpoint = match[-1]\n",
    "                                    http_method = match[1] if len(match) > 2 else \"unknown\"\n",
    "                                    file_endpoints.append({\n",
    "                                        'endpoint': endpoint,\n",
    "                                        'method': http_method,\n",
    "                                        'pattern_type': pattern_name\n",
    "                                    })\n",
    "                            else:\n",
    "                                # Handle string results\n",
    "                                for match in matches:\n",
    "                                    file_endpoints.append({\n",
    "                                        'endpoint': match,\n",
    "                                        'method': \"unknown\",\n",
    "                                        'pattern_type': pattern_name\n",
    "                                    })\n",
    "                    \n",
    "                    if file_endpoints:\n",
    "                        results['endpoints'].append({\n",
    "                            'file': current_file,\n",
    "                            'endpoints': file_endpoints\n",
    "                        })\n",
    "                    \n",
    "                    # Detect request/response models\n",
    "                    for pattern_name, pattern in body_patterns.items():\n",
    "                        matches = re.findall(pattern, file_content)\n",
    "                        if matches:\n",
    "                            for match in matches:\n",
    "                                if isinstance(match, tuple):\n",
    "                                    name = match[0]\n",
    "                                    body = match[1]\n",
    "                                else:\n",
    "                                    name = pattern_name\n",
    "                                    body = match\n",
    "                                \n",
    "                                results['request_response_models'].append({\n",
    "                                    'file': current_file,\n",
    "                                    'type': pattern_name,\n",
    "                                    'name': name,\n",
    "                                    'content': body.strip()\n",
    "                                })\n",
    "        \n",
    "        # Determine the most likely framework\n",
    "        if results['framework_scores']:\n",
    "            most_likely_framework = max(results['framework_scores'].items(), key=lambda x: x[1])\n",
    "            results['detected_framework'] = {\n",
    "                'name': most_likely_framework[0],\n",
    "                'confidence': most_likely_framework[1]\n",
    "            }\n",
    "        else:\n",
    "            results['detected_framework'] = {\n",
    "                'name': 'unknown',\n",
    "                'confidence': 0\n",
    "            }\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing repository: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Analyze the consolidated repository\n",
    "analysis_results = analyze_consolidated_repo()\n",
    "\n",
    "# Print summary of results\n",
    "if analysis_results:\n",
    "    print(f\"Detected Framework: {analysis_results['detected_framework']['name']} (confidence: {analysis_results['detected_framework']['confidence']})\")\n",
    "    print(f\"Found {sum(len(item['endpoints']) for item in analysis_results['endpoints'])} endpoints across {len(analysis_results['endpoints'])} files\")\n",
    "    print(f\"Found {len(analysis_results['request_response_models'])} potential request/response models\")\n",
    "    \n",
    "    # Save detailed results to JSON\n",
    "    with open('api_analysis_results.json', 'w') as f:\n",
    "        json.dump(analysis_results, f, indent=2)\n",
    "    print(\"Detailed results saved to api_analysis_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import json\n",
    "import os\n",
    "\n",
    "def analyze_with_llama(consolidated_file=\"consolidated_repo.txt\", llama_model_path=\"models/llama-2-7b-chat.gguf\"):\n",
    "    \"\"\"\n",
    "    Use LLaMA to analyze the codebase for endpoints, request/response bodies, and framework detection\n",
    "    \"\"\"\n",
    "    # Check if LLaMA model exists\n",
    "    if not os.path.exists(llama_model_path):\n",
    "        print(f\"LLaMA model not found at {llama_model_path}. Please download it first.\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize LLaMA model\n",
    "    llm = LlamaCpp(\n",
    "        model_path=llama_model_path,\n",
    "        temperature=0.1,\n",
    "        max_tokens=2000,\n",
    "        top_p=0.95,\n",
    "        n_ctx=4096  # Context window size\n",
    "    )\n",
    "    \n",
    "    # Read the consolidated file in chunks due to context window limitations\n",
    "    with open(consolidated_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Split content into manageable chunks (e.g., by file)\n",
    "    file_sections = re.split(r'={80}\\nFILE:\\s*(.+?)\\n={80}', content)\n",
    "    \n",
    "    # Prepare results container\n",
    "    llama_results = {\n",
    "        'framework_analysis': {},\n",
    "        'endpoints': [],\n",
    "        'data_models': []\n",
    "    }\n",
    "    \n",
    "    # Process each file with LLaMA\n",
    "    for i in range(1, len(file_sections), 2):\n",
    "        if i < len(file_sections):\n",
    "            current_file = file_sections[i]\n",
    "            file_content = file_sections[i+1] if i+1 < len(file_sections) else \"\"\n",
    "            \n",
    "            # Skip if file content is too large\n",
    "            if len(file_content) > 3500:\n",
    "                print(f\"Skipping {current_file} - too large for context window\")\n",
    "                continue\n",
    "            \n",
    "            # Create prompt for framework detection\n",
    "            framework_prompt = PromptTemplate(\n",
    "                input_variables=[\"code\"],\n",
    "                template=\"\"\"\n",
    "                Analyze this code and identify the web framework being used. \n",
    "                Focus on imports, decorators, and patterns specific to frameworks like React, Angular, Vue, Express, Django, Flask, Spring, ASP.NET, or Laravel.\n",
    "                \n",
    "                Code:\n",
    "                ```\n",
    "                {code}\n",
    "                ```\n",
    "                \n",
    "                Respond with a JSON object with these fields:\n",
    "                1. \"framework\": the name of the detected framework\n",
    "                2. \"confidence\": a number from 0-100 indicating confidence\n",
    "                3. \"evidence\": key patterns that led to this conclusion\n",
    "                \"\"\"\n",
    "            )\n",
    "            \n",
    "            # Create prompt for endpoint detection\n",
    "            endpoint_prompt = PromptTemplate(\n",
    "                input_variables=[\"code\", \"filename\"],\n",
    "                template=\"\"\"\n",
    "                Analyze this code and extract all API endpoints. \n",
    "                Look for route definitions, API controllers, and endpoint handlers.\n",
    "                \n",
    "                Filename: {filename}\n",
    "                \n",
    "                Code:\n",
    "                ```\n",
    "                {code}\n",
    "                ```\n",
    "                \n",
    "                Respond with a JSON array of endpoints, where each endpoint has:\n",
    "                1. \"path\": the URL path\n",
    "                2. \"method\": HTTP method (GET, POST, etc.)\n",
    "                3. \"description\": brief description of what the endpoint does\n",
    "                4. \"request_body\": description of request parameters (if identifiable)\n",
    "                5. \"response_body\": description of response format (if identifiable)\n",
    "                \"\"\"\n",
    "            )\n",
    "            \n",
    "            # Execute LLaMA for framework detection\n",
    "            try:\n",
    "                framework_chain = LLMChain(llm=llm, prompt=framework_prompt)\n",
    "                framework_response = framework_chain.run(code=file_content[:3500])\n",
    "                \n",
    "                # Parse JSON response\n",
    "                try:\n",
    "                    framework_data = json.loads(framework_response)\n",
    "                    llama_results['framework_analysis'][current_file] = framework_data\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Failed to parse framework analysis for {current_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in framework analysis for {current_file}: {str(e)}\")\n",
    "            \n",
    "            # Execute LLaMA for endpoint detection\n",
    "            try:\n",
    "                endpoint_chain = LLMChain(llm=llm, prompt=endpoint_prompt)\n",
    "                endpoint_response = endpoint_chain.run(code=file_content[:3500], filename=current_file)\n",
    "                \n",
    "                # Parse JSON response\n",
    "                try:\n",
    "                    endpoint_data = json.loads(endpoint_response)\n",
    "                    if endpoint_data:\n",
    "                        llama_results['endpoints'].append({\n",
    "                            'file': current_file,\n",
    "                            'endpoints': endpoint_data\n",
    "                        })\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Failed to parse endpoint analysis for {current_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in endpoint analysis for {current_file}: {str(e)}\")\n",
    "    \n",
    "    # Determine overall framework based on individual file analyses\n",
    "    framework_votes = defaultdict(int)\n",
    "    for file_analysis in llama_results['framework_analysis'].values():\n",
    "        if 'framework' in file_analysis and file_analysis['framework'] != 'unknown':\n",
    "            framework_votes[file_analysis['framework']] += file_analysis.get('confidence', 50)\n",
    "    \n",
    "    if framework_votes:\n",
    "        most_likely_framework = max(framework_votes.items(), key=lambda x: x[1])\n",
    "        llama_results['detected_framework'] = {\n",
    "            'name': most_likely_framework[0],\n",
    "            'confidence': most_likely_framework[1] / sum(framework_votes.values()) * 100\n",
    "        }\n",
    "    else:\n",
    "        llama_results['detected_framework'] = {\n",
    "            'name': 'unknown',\n",
    "            'confidence': 0\n",
    "        }\n",
    "    \n",
    "    # Save results to JSON\n",
    "    with open('llama_api_analysis.json', 'w') as f:\n",
    "        json.dump(llama_results, f, indent=2)\n",
    "    \n",
    "    print(f\"LLaMA analysis complete. Detected framework: {llama_results['detected_framework']['name']}\")\n",
    "    print(f\"Found {sum(len(item['endpoints']) for item in llama_results['endpoints'])} endpoints\")\n",
    "    print(\"Detailed results saved to llama_api_analysis.json\")\n",
    "    \n",
    "    return llama_results\n",
    "\n",
    "# Uncomment to run LLaMA analysis (requires LLaMA model)\n",
    "# llama_analysis = analyze_with_llama()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_api_analysis(consolidated_file=\"consolidated_repo.txt\", use_llama=False, llama_model_path=\"models/llama-2-7b-chat.gguf\"):\n",
    "    \"\"\"\n",
    "    Perform comprehensive API analysis using both pattern matching and optionally LLaMA\n",
    "    \"\"\"\n",
    "    # Run pattern-based analysis\n",
    "    pattern_results = analyze_consolidated_repo(consolidated_file)\n",
    "    \n",
    "    # Run LLaMA analysis if requested\n",
    "    llama_results = None\n",
    "    if use_llama:\n",
    "        llama_results = analyze_with_llama(consolidated_file, llama_model_path)\n",
    "    \n",
    "    # Combine results\n",
    "    combined_results = {\n",
    "        'pattern_analysis': pattern_results,\n",
    "        'llama_analysis': llama_results if llama_results else \"Not performed\"\n",
    "    }\n",
    "    \n",
    "    # Determine final framework\n",
    "    if llama_results and llama_results['detected_framework']['confidence'] > 50:\n",
    "        final_framework = llama_results['detected_framework']['name']\n",
    "    else:\n",
    "        final_framework = pattern_results['detected_framework']['name']\n",
    "    \n",
    "    combined_results['final_framework'] = final_framework\n",
    "    \n",
    "    # Save combined results\n",
    "    with open('combined_api_analysis.json', 'w') as f:\n",
    "        json.dump(combined_results, f, indent=2)\n",
    "    \n",
    "    print(f\"Comprehensive analysis complete.\")\n",
    "    print(f\"Detected framework: {final_framework}\")\n",
    "    print(\"Detailed results save\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
